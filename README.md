# Description

## 1. TFIDF:
This project focuses on the classification of documents into five categories—sport, business, politics, entertainment, and tech—using machine learning techniques. The process begins with text preprocessing, including converting documents to lowercase, removing punctuation, tokenizing, removing stop words, and applying stemming. Only words present in a predefined dictionary were considered for further analysis. A custom Term Frequency-Inverse Document Frequency (TF-IDF) matrix was computed to represent the importance of words across the corpus. The TF-IDF scores were then used as features to train a Random Forest classifier, with hyperparameters optimized using GridSearchCV. The project outputs include a 1000x1000 TF-IDF matrix, a list of the top 3 most frequent words, and the top 3 highest average TF-IDF words for each category, all encapsulated in JSON format. The code, along with all relevant files, is included in the repository for easy access and replication.

## 2. DocumentClassification:
This assignment involved classifying documents within a corpus using tree-based models. The task required training and evaluating various models, including decision trees and random forests, with a focus on optimizing parameters through techniques such as GridSearchCV. The dataset consisted of news articles categorized into five distinct classes: sport, business, politics, entertainment, and tech. Key steps included preprocessing the data using methods like TF-IDF, applying cross-validation to assess model performance, and tuning hyperparameters for optimal accuracy. The final model was then used to predict the categories of unseen test data, and the results were compiled for submission.

## 3. DifferentMethods:
This project aims to classify news articles into five categories: sport, business, politics, entertainment, and tech. Using a provided dataset of 1063 labeled news articles, various feature extraction techniques such as n-grams, TFIDF, and word embeddings (e.g., GloVe, BERT) were explored. A neural network with two hidden layers of 128 neurons was trained and evaluated using 5-fold cross-validation. The performance was optimized using different learning rates and optimizers, and the best model was applied to predict the labels for 735 test articles. Results were analyzed based on accuracy metrics, and a final report along with the code and predicted labels were submitted.
