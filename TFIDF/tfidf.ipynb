{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ef2b4e2c-0bac-4622-985f-dc0d1cd6dbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "df = pd.read_csv('24_train_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a0bf445-354f-4dfe-95ce-3214d4b1f285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1429</td>\n",
       "      <td>sfa awaits report over mikoliunas the scottish...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>parmalat to return to stockmarket parmalat  th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1633</td>\n",
       "      <td>edu blasts arsenal arsenal s brazilian midfiel...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2178</td>\n",
       "      <td>henman decides to quit davis cup tim henman ha...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194</td>\n",
       "      <td>french suitor holds lse meeting european stock...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1250</td>\n",
       "      <td>blair  damaged  by blunkett row a majority of ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1639</td>\n",
       "      <td>a november to remember last saturday  one news...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>916</td>\n",
       "      <td>highbury tunnel players in clear the football ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2217</td>\n",
       "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>902</td>\n",
       "      <td>eastwood s baby scoops top oscars clint eastwo...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text  \\\n",
       "0         1429  sfa awaits report over mikoliunas the scottish...   \n",
       "1         1896  parmalat to return to stockmarket parmalat  th...   \n",
       "2         1633  edu blasts arsenal arsenal s brazilian midfiel...   \n",
       "3         2178  henman decides to quit davis cup tim henman ha...   \n",
       "4          194  french suitor holds lse meeting european stock...   \n",
       "..         ...                                                ...   \n",
       "995       1250  blair  damaged  by blunkett row a majority of ...   \n",
       "996       1639  a november to remember last saturday  one news...   \n",
       "997        916  highbury tunnel players in clear the football ...   \n",
       "998       2217  top stars join us tsunami tv show brad pitt  r...   \n",
       "999        902  eastwood s baby scoops top oscars clint eastwo...   \n",
       "\n",
       "          Category  \n",
       "0            sport  \n",
       "1         business  \n",
       "2            sport  \n",
       "3            sport  \n",
       "4         business  \n",
       "..             ...  \n",
       "995       politics  \n",
       "996          sport  \n",
       "997          sport  \n",
       "998  entertainment  \n",
       "999  entertainment  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "681413fa-a667-4349-92d9-16eaf3cfc8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gayathrisjs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gayathrisjs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "# import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# stemming tool from nltk\n",
    "stemmer = PorterStemmer()\n",
    "# a mapping dictionary that help remove punctuations\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "def get_tokens(text):\n",
    "  # turn document into lowercase\n",
    "  lowers = text.lower()\n",
    "  # remove punctuations\n",
    "  no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "  # tokenize document\n",
    "  tokens = nltk.word_tokenize(no_punctuation)\n",
    "  # remove stop words\n",
    "  filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "  # stemming process\n",
    "  stemmed = []\n",
    "  for item in filtered:\n",
    "      stemmed.append(stemmer.stem(item))\n",
    "  # final unigrams\n",
    "  return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5d3177f4-345b-4445-8db9-894ac3d8492b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']=df['Text'].apply(lambda row: get_tokens(row))\n",
    "#df1=df['Text'].apply(lambda row: get_tokens(row.to_string(index=False)), axis=1)\n",
    "# df['tokens']=df.apply(lambda x: get_tokens(''.join(x)),axis=1)\n",
    "# df[['ArticleId', 'Text', 'Category']] = df.apply(lambda row: row.apply(get_tokens), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97bef1ba-addc-4b4e-9858-6f5b035b7b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1429</td>\n",
       "      <td>sfa awaits report over mikoliunas the scottish...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[sfa, await, report, mikoliuna, scottish, foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>parmalat to return to stockmarket parmalat  th...</td>\n",
       "      <td>business</td>\n",
       "      <td>[parmalat, return, stockmarket, parmalat, ital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1633</td>\n",
       "      <td>edu blasts arsenal arsenal s brazilian midfiel...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[edu, blast, arsen, arsen, brazilian, midfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2178</td>\n",
       "      <td>henman decides to quit davis cup tim henman ha...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[henman, decid, quit, davi, cup, tim, henman, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194</td>\n",
       "      <td>french suitor holds lse meeting european stock...</td>\n",
       "      <td>business</td>\n",
       "      <td>[french, suitor, hold, lse, meet, european, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1250</td>\n",
       "      <td>blair  damaged  by blunkett row a majority of ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[blair, damag, blunkett, row, major, voter, 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1639</td>\n",
       "      <td>a november to remember last saturday  one news...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[novemb, rememb, last, saturday, one, newspap,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>916</td>\n",
       "      <td>highbury tunnel players in clear the football ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[highburi, tunnel, player, clear, footbal, ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2217</td>\n",
       "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[top, star, join, us, tsunami, tv, show, brad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>902</td>\n",
       "      <td>eastwood s baby scoops top oscars clint eastwo...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[eastwood, babi, scoop, top, oscar, clint, eas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text  \\\n",
       "0         1429  sfa awaits report over mikoliunas the scottish...   \n",
       "1         1896  parmalat to return to stockmarket parmalat  th...   \n",
       "2         1633  edu blasts arsenal arsenal s brazilian midfiel...   \n",
       "3         2178  henman decides to quit davis cup tim henman ha...   \n",
       "4          194  french suitor holds lse meeting european stock...   \n",
       "..         ...                                                ...   \n",
       "995       1250  blair  damaged  by blunkett row a majority of ...   \n",
       "996       1639  a november to remember last saturday  one news...   \n",
       "997        916  highbury tunnel players in clear the football ...   \n",
       "998       2217  top stars join us tsunami tv show brad pitt  r...   \n",
       "999        902  eastwood s baby scoops top oscars clint eastwo...   \n",
       "\n",
       "          Category                                             tokens  \n",
       "0            sport  [sfa, await, report, mikoliuna, scottish, foot...  \n",
       "1         business  [parmalat, return, stockmarket, parmalat, ital...  \n",
       "2            sport  [edu, blast, arsen, arsen, brazilian, midfield...  \n",
       "3            sport  [henman, decid, quit, davi, cup, tim, henman, ...  \n",
       "4         business  [french, suitor, hold, lse, meet, european, st...  \n",
       "..             ...                                                ...  \n",
       "995       politics  [blair, damag, blunkett, row, major, voter, 68...  \n",
       "996          sport  [novemb, rememb, last, saturday, one, newspap,...  \n",
       "997          sport  [highburi, tunnel, player, clear, footbal, ass...  \n",
       "998  entertainment  [top, star, join, us, tsunami, tv, show, brad,...  \n",
       "999  entertainment  [eastwood, babi, scoop, top, oscar, clint, eas...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98777c34-afb2-4003-88f3-16fbe7bf7078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('dictionary.txt','r') as dicti:\n",
    "    dictionary= set(dicti.read().splitlines())\n",
    "with open('dictionary.txt','r') as dicti:\n",
    "    dictionary_list= list(dicti.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "67ce4591-a399-43f3-a38a-26a4064ff019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '1', '10', '100', '11', '12', '13', '14', '15', '16', '18', '2', '20', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '24', '25', '3', '30', '4', '40', '5', '50', 'abil', 'abl', 'accept', 'access', 'accord', 'account', 'accus', 'achiev', 'across', 'act', 'action', 'activ', 'actor', 'actress', 'ad', 'add', 'address', 'admit', 'affect', 'africa', 'age', 'agenc', 'ago', 'agre', 'agreement', 'ahead', 'aid', 'aim', 'air', 'airlin', 'album', 'alleg', 'allow', 'almost', 'alreadi', 'also', 'although', 'alway', 'america', 'american', 'among', 'amount', 'analyst', 'announc', 'annual', 'anoth', 'answer', 'anyth', 'appeal', 'appear', 'appl', 'approach', 'approv', 'april', 'area', 'argu', 'around', 'arrest', 'arsen', 'artist', 'ask', 'associ', 'asylum', 'athlet', 'attack', 'attempt', 'attend', 'attract', 'audienc', 'australia', 'australian', 'author', 'avail', 'averag', 'aviat', 'avoid', 'award', 'away', 'back', 'bad', 'ball', 'ban', 'band', 'bank', 'bankruptci', 'base', 'battl', 'bbc', 'beat', 'becam', 'becom', 'begin', 'behind', 'believ', 'benefit', 'best', 'better', 'bid', 'big', 'biggest', 'bill', 'billion', 'bit', 'black', 'blair', 'blog', 'board', 'bodi', 'book', 'boost', 'boss', 'bought', 'box', 'brand', 'break', 'bring', 'britain', 'british', 'broadband', 'broadcast', 'brown', 'bt', 'budget', 'build', 'bush', 'busi', 'buy', 'call', 'came', 'camera', 'campaign', 'campbel', 'car', 'card', 'care', 'career', 'carri', 'case', 'cash', 'categori', 'caus', 'celebr', 'centr', 'central', 'ceremoni', 'chairman', 'challeng', 'champion', 'championship', 'chanc', 'chancellor', 'chang', 'channel', 'charg', 'charl', 'chart', 'chelsea', 'chief', 'children', 'china', 'choic', 'christma', 'citi', 'claim', 'clark', 'clear', 'close', 'club', 'coach', 'code', 'collect', 'combin', 'come', 'comedi', 'comment', 'commiss', 'commit', 'committe', 'common', 'commun', 'compani', 'compar', 'compet', 'competit', 'complet', 'comput', 'concern', 'confer', 'confid', 'confirm', 'connect', 'conserv', 'consid', 'consol', 'consum', 'content', 'contest', 'continu', 'contract', 'control', 'controversi', 'copi', 'corpor', 'cost', 'could', 'council', 'countri', 'coupl', 'cours', 'court', 'creat', 'credit', 'crime', 'crimin', 'critic', 'criticis', 'cup', 'current', 'custom', 'cut', 'damag', 'data', 'date', 'davi', 'david', 'day', 'de', 'deal', 'death', 'debat', 'debt', 'debut', 'decemb', 'decid', 'decis', 'declin', 'defeat', 'defenc', 'defend', 'deficit', 'deliv', 'dem', 'demand', 'democrat', 'deni', 'depart', 'describ', 'design', 'despit', 'detail', 'develop', 'devic', 'die', 'differ', 'difficult', 'digit', 'direct', 'director', 'disappoint', 'disast', 'discuss', 'distribut', 'dollar', 'domin', 'done', 'doubl', 'doubt', 'download', 'dr', 'drive', 'drop', 'drug', 'due', 'dvd', 'earli', 'earlier', 'earn', 'econom', 'economi', 'educ', 'effect', 'effort', 'eight', 'either', 'elect', 'electron', 'email', 'emerg', 'employ', 'end', 'engin', 'england', 'enough', 'ensur', 'enter', 'entertain', 'estim', 'eu', 'euro', 'europ', 'european', 'even', 'event', 'ever', 'everi', 'everyon', 'everyth', 'evid', 'exampl', 'exchang', 'execut', 'exist', 'expect', 'experi', 'expert', 'explain', 'export', 'extra', 'face', 'fact', 'fail', 'fall', 'famili', 'fan', 'far', 'favour', 'favourit', 'fear', 'featur', 'februari', 'feder', 'feel', 'fell', 'felt', 'festiv', 'field', 'fight', 'figur', 'file', 'film', 'final', 'financ', 'financi', 'find', 'fine', 'finish', 'firm', 'first', 'fit', 'five', 'follow', 'footbal', 'forc', 'forecast', 'foreign', 'form', 'former', 'forward', 'found', 'four', 'fourth', 'franc', 'fraud', 'free', 'french', 'friday', 'friend', 'front', 'full', 'fund', 'futur', 'gadget', 'gain', 'game', 'gave', 'gener', 'german', 'germani', 'get', 'giant', 'give', 'given', 'global', 'go', 'goal', 'gold', 'golden', 'gone', 'good', 'googl', 'gordon', 'got', 'govern', 'grand', 'great', 'ground', 'group', 'grow', 'growth', 'half', 'hand', 'handset', 'happen', 'happi', 'hard', 'head', 'health', 'hear', 'held', 'help', 'high', 'higher', 'histori', 'hit', 'hold', 'hollywood', 'home', 'honour', 'hope', 'host', 'hour', 'hous', 'howard', 'howev', 'huge', 'human', 'hunt', 'id', 'idea', 'illeg', 'imag', 'immigr', 'impact', 'import', 'impress', 'improv', 'includ', 'incom', 'increas', 'independ', 'india', 'indian', 'individu', 'industri', 'inflat', 'inform', 'initi', 'injuri', 'insist', 'instead', 'interest', 'intern', 'internet', 'introduc', 'invest', 'investig', 'investor', 'involv', 'iraq', 'ireland', 'irish', 'issu', 'itali', 'jame', 'januari', 'japan', 'job', 'john', 'johnson', 'join', 'jone', 'judg', 'june', 'keep', 'kennedi', 'key', 'kick', 'know', 'known', 'labour', 'lack', 'larg', 'largest', 'last', 'late', 'later', 'latest', 'launch', 'law', 'lawyer', 'lead', 'leader', 'leagu', 'least', 'leav', 'led', 'left', 'legal', 'less', 'let', 'level', 'lib', 'liber', 'life', 'light', 'like', 'limit', 'line', 'link', 'list', 'listen', 'littl', 'live', 'liverpool', 'local', 'london', 'long', 'look', 'lord', 'lose', 'loss', 'lost', 'lot', 'love', 'low', 'lower', 'machin', 'made', 'magazin', 'main', 'major', 'make', 'maker', 'man', 'manag', 'manchest', 'mani', 'manufactur', 'march', 'mark', 'market', 'martin', 'match', 'matter', 'may', 'mean', 'meanwhil', 'measur', 'media', 'meet', 'member', 'men', 'messag', 'met', 'michael', 'microsoft', 'might', 'mike', 'million', 'mini', 'minist', 'minut', 'miss', 'mobil', 'model', 'moment', 'monday', 'money', 'monitor', 'month', 'move', 'movi', 'mp', 'mr', 'ms', 'much', 'music', 'must', 'name', 'nation', 'need', 'net', 'network', 'never', 'new', 'news', 'newspap', 'next', 'night', 'nomin', 'north', 'noth', 'novemb', 'number', 'octob', 'offer', 'offic', 'offici', 'often', 'oil', 'old', 'olymp', 'one', 'onlin', 'open', 'oper', 'opportun', 'opposit', 'order', 'organis', 'origin', 'oscar', 'other', 'outsid', 'owner', 'paid', 'pair', 'paper', 'parent', 'park', 'parliament', 'part', 'parti', 'particularli', 'pass', 'past', 'paul', 'pay', 'pc', 'penalti', 'pension', 'peopl', 'per', 'perform', 'period', 'person', 'peter', 'phone', 'pick', 'pictur', 'place', 'plan', 'play', 'player', 'pledg', 'point', 'polic', 'polici', 'polit', 'poll', 'poor', 'pop', 'popular', 'portabl', 'posit', 'possibl', 'post', 'potenti', 'power', 'predict', 'premiership', 'prepar', 'present', 'presid', 'press', 'pressur', 'previou', 'price', 'prime', 'privat', 'prize', 'probabl', 'problem', 'process', 'produc', 'product', 'profit', 'program', 'programm', 'project', 'promis', 'promot', 'properti', 'propos', 'protect', 'prove', 'provid', 'public', 'publish', 'push', 'put', 'qualiti', 'quarter', 'question', 'quit', 'race', 'radio', 'rais', 'rang', 'rate', 'rather', 'reach', 'read', 'real', 'realli', 'reason', 'receiv', 'recent', 'record', 'reduc', 'refere', 'reflect', 'reform', 'refus', 'region', 'reject', 'releas', 'remain', 'replac', 'report', 'repres', 'requir', 'research', 'respect', 'respond', 'respons', 'rest', 'result', 'retail', 'return', 'reveal', 'revenu', 'richard', 'right', 'rise', 'risk', 'rival', 'road', 'robert', 'robinson', 'rock', 'roddick', 'role', 'rose', 'round', 'row', 'rugbi', 'rule', 'run', 'russia', 'russian', 'said', 'sale', 'saturday', 'save', 'saw', 'say', 'scheme', 'school', 'score', 'scotland', 'scottish', 'screen', 'search', 'season', 'seat', 'second', 'secretari', 'sector', 'secur', 'see', 'seed', 'seek', 'seem', 'seen', 'sell', 'send', 'senior', 'sent', 'septemb', 'seri', 'seriou', 'serv', 'servic', 'set', 'seven', 'sever', 'share', 'sharehold', 'short', 'shot', 'show', 'shown', 'side', 'sign', 'signific', 'similar', 'simpli', 'sinc', 'singer', 'singl', 'sir', 'site', 'situat', 'six', 'slow', 'small', 'softwar', 'sold', 'someth', 'song', 'soni', 'soon', 'sound', 'sourc', 'south', 'spain', 'spam', 'speak', 'special', 'specul', 'speech', 'speed', 'spend', 'spent', 'spokesman', 'sport', 'squad', 'stage', 'stand', 'standard', 'star', 'start', 'state', 'statement', 'station', 'stay', 'step', 'still', 'stock', 'stop', 'store', 'stori', 'street', 'strong', 'struggl', 'student', 'studi', 'studio', 'success', 'suffer', 'suggest', 'summer', 'sunday', 'support', 'sure', 'surpris', 'survey', 'system', 'tackl', 'take', 'taken', 'talk', 'target', 'tax', 'team', 'technolog', 'televis', 'tell', 'term', 'terror', 'test', 'theatr', 'thing', 'think', 'third', 'though', 'thought', 'thousand', 'threat', 'three', 'thursday', 'time', 'titl', 'today', 'togeth', 'told', 'toni', 'took', 'tool', 'top', 'tori', 'total', 'tough', 'tour', 'track', 'trade', 'train', 'travel', 'tri', 'trial', 'trust', 'tsunami', 'tuesday', 'turn', 'tv', 'two', 'uk', 'ukip', 'understand', 'union', 'unit', 'univers', 'unlik', 'unveil', 'us', 'use', 'user', 'v', 'valu', 'version', 'via', 'victim', 'victori', 'video', 'view', 'viru', 'visit', 'vote', 'voter', 'wage', 'wait', 'wale', 'want', 'war', 'warn', 'watch', 'way', 'web', 'websit', 'wednesday', 'week', 'weekend', 'well', 'went', 'west', 'whether', 'whole', 'whose', 'wide', 'william', 'win', 'window', 'winner', 'within', 'without', 'women', 'word', 'work', 'worker', 'world', 'worri', 'worth', 'would', 'write', 'wrong', 'year', 'yet', 'york', 'young', 'yuko', 'zealand']\n"
     ]
    }
   ],
   "source": [
    "# len(dictionary)\n",
    "print(dictionary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96f824de-42cb-4160-9e22-f80408b6dd56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ArticleId', 'Text', 'Category', 'tokens'], dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4f0be6f6-b58c-40e0-8312-cfd072eb9dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['filtered tokens']= df['tokens'].apply(lambda row: [text for text in row if text in dictionary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e5d15ab1-6068-40db-b3e4-824978bd2c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tfidf_matrix(rows, dictionary_list):\n",
    "    n=len(rows)\n",
    "    m=len(dictionary)\n",
    "    \n",
    "    tfidf=np.zeros((n,m))\n",
    "    idf_j={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # word_doc_frequency= {dic: sum(1 for row)}\n",
    "    for dic in dictionary_list:\n",
    "        count= rows.apply(lambda row: 1 if dic in row else 0)\n",
    "        total_doc_count= count.sum()\n",
    "        idf_j[dic]= np.log(n/total_doc_count)\n",
    "    \n",
    "    print(count.sum(),len(count))\n",
    "    \n",
    "    for i,row in enumerate(rows):\n",
    "        #print('row=',row)\n",
    "        frequency_word= {w: row.count(w) for w in row}\n",
    "        frequency_max= max(frequency_word.values())\n",
    "        # print(i, frequency_max,frequency_word)\n",
    "        for j,dic in enumerate(dictionary_list):\n",
    "            tf_ij=frequency_word.get(dic,0)/frequency_max\n",
    "            tfidf[i,j]=tf_ij *idf_j[dic]\n",
    "    return tfidf,idf_j\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86f1f9df-b852-49db-8b80-5cb7c3264d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 1000\n"
     ]
    }
   ],
   "source": [
    "tfidf,idf_j=tfidf_matrix(df['filtered tokens'],dictionary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf7f4a56-9329-49ef-8634-4d5df3b330f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28878469, 0.        , 0.36036196, ..., 0.51540439, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.15158953, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8d0de427-e450-4ad9-97d7-764847897a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename='matrix.txt'\n",
    "np.savetxt(filename, tfidf, fmt='%f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6f02f50b-05f8-4281-9b48-a4de928b3995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sport', 'business', 'tech', 'entertainment', 'politics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = df['Category'].unique()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9c9f1166-a211-4297-b895-844466693779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sport': {'said': 428, 'game': 353, 'win': 288}, 'business': {'said': 724, 'us': 377, 'year': 360}, 'tech': {'said': 757, 'use': 459, 'peopl': 427}, 'entertainment': {'film': 450, 'said': 386, 'year': 249}, 'politics': {'said': 996, 'mr': 726, 'would': 495}}\n"
     ]
    }
   ],
   "source": [
    "count={}\n",
    "for category in categories:\n",
    "    count[category] = {}\n",
    "    category_df = df[df['Category'] == category]\n",
    "    # break\n",
    "    summ=0\n",
    "    for word in dictionary:\n",
    "        summ=0\n",
    "        for row in category_df['filtered tokens']:\n",
    "            summ+= row.count(word)\n",
    "        count[category][word]=summ\n",
    "    sorted_list = sorted(count[category].items(), key=lambda item: -item[1])\n",
    "    sorted_list = sorted_list[:3]\n",
    "    count[category] = dict(sorted_list)\n",
    "print(count)\n",
    "with open(\"frequency.json\", \"w\") as outfile:\n",
    "    json.dump(count, outfile)\n",
    "        \n",
    "        # count[category][word]=category_df['Text'].apply(lambda row: sum(1 if word in row else 0))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d37c4f6-6b09-4059-9eae-fb9b07ed40b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sport': {'said': 428, 'game': 353, 'win': 288},\n",
       " 'business': {'said': 724, 'us': 377, 'year': 360},\n",
       " 'tech': {'said': 757, 'use': 459, 'peopl': 427},\n",
       " 'entertainment': {'film': 450, 'said': 386, 'year': 249},\n",
       " 'politics': {'said': 996, 'mr': 726, 'would': 495}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf38584d-9a0f-4790-8b9c-d97db5681016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_dic={}\n",
    "for category in categories:\n",
    "    category_dic[category]={}\n",
    "    category_loc= df[df['Category']==category].index\n",
    "    #print(category,category_loc)\n",
    "    category_matrix=tfidf[category_loc]\n",
    "    #print(category,category_matrix)\n",
    "    req_average=np.mean(category_matrix,axis=0)\n",
    "    # top_ind= np.argsort(req_average)[-3: ][::-1]\n",
    "    mapped_list = []\n",
    "    for index, value in enumerate(req_average):\n",
    "        mapped_list.append((value, index))\n",
    "    mapped_list.sort(key = lambda m : m[0], reverse = True)\n",
    "    mapped_lis = mapped_list[:3]\n",
    "    for ind in mapped_lis:\n",
    "        category_dic[category][dictionary_list[ind[1]]]= ind[0]\n",
    "with open(\"score.json\", \"w\") as outfile:\n",
    "    json.dump(category_dic, outfile)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "75a4bd2d-9583-402a-8a2a-038ae721c657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.45105036714182084, 507),\n",
       " (0.4313731783204545, 312),\n",
       " (0.42043597469422206, 601)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e1be0299-30b4-41c9-8ab6-10de501d3f07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sport': {'game': 0.35727414648708805,\n",
       "  'england': 0.31907434737608514,\n",
       "  'win': 0.30741067997068455},\n",
       " 'business': {'firm': 0.2891252868078186,\n",
       "  'bank': 0.2697288199539767,\n",
       "  'market': 0.2616290834155383},\n",
       " 'tech': {'mobil': 0.3462714837303001,\n",
       "  'phone': 0.3319065027131584,\n",
       "  'softwar': 0.3152238172837377},\n",
       " 'entertainment': {'film': 0.7216412939111394,\n",
       "  'award': 0.4106447057087541,\n",
       "  'star': 0.40803563438879187},\n",
       " 'politics': {'labour': 0.45105036714182084,\n",
       "  'elect': 0.4313731783204545,\n",
       "  'mr': 0.42043597469422206}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "46dfe7c8-554f-4e82-8636-a8b115880720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'avail'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b213617e-dbf3-43f7-986f-25b7c16eb516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30941217, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.86635408, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 1.71801463, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.20627478, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.22214207, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.21810799, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238b8b1-69e3-47ea-8035-dac34d3da0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
